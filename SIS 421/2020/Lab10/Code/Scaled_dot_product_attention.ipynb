{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaled dot-product attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEvaqXOwtRAZO8Zvo+TkxN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zelechos/IA_Code/blob/master/Lab10/Scaled_dot_product_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "running-pastor"
      },
      "source": [
        "Vamos ahora a resolver el problema utilizando el mecanismo de atención descrito anteriormente. Lo primero que tenemos que tener en cuenta es que los mecanismos de atención funcionan con conjuntos de vectores (secuencia), por lo que tenemos que reinterpretar nuestras imágenes. Para ello, vamos a dividirlas en 16 *patches* de 7x7. De esta manera, nuestras imágenes ahora serán secuencias de *patches* con las que nuestro mecanismo de atención podrá trabajar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T08:49:17.830379Z",
          "start_time": "2021-03-24T08:49:16.241378Z"
        },
        "id": "veterinary-bahrain"
      },
      "source": [
        "#import pytorch_lightning as pl\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T08:49:17.846378Z",
          "start_time": "2021-03-24T08:49:17.831379Z"
        },
        "code_folding": [],
        "id": "facial-learning"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X \n",
        "    self.y = y \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, ix):\n",
        "    return torch.tensor(self.X[ix]).float(), torch.tensor(self.y[ix]).long()\n",
        "\n",
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, batch_size: int = 64, Dataset = Dataset):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.Dataset = Dataset\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        mnist = fetch_openml('mnist_784', version=1)\n",
        "        X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "        X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., y[:60000].astype(np.int), y[60000:].astype(np.int)\n",
        "        self.train_ds = self.Dataset(X_train, y_train)\n",
        "        self.val_ds = self.Dataset(X_test, y_test)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_ds, batch_size=self.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T08:54:53.715129Z",
          "start_time": "2021-03-24T08:54:53.695110Z"
        },
        "id": "incorporate-occurrence"
      },
      "source": [
        "class AttnDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y, patch_size=(7, 7)):\n",
        "    self.X = X \n",
        "    self.y = y \n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, ix):\n",
        "    image = torch.tensor(self.X[ix]).float().view(28, 28) # 28 x 28\n",
        "    h, w = self.patch_size\n",
        "    patches = image.unfold(0, h, h).unfold(1, w, w) # 4 x 4 x 7 x 7\n",
        "    patches = patches.contiguous().view(-1, h*w) # 16 x 49\n",
        "    return patches, torch.tensor(self.y[ix]).long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T08:55:12.621360Z",
          "start_time": "2021-03-24T08:54:53.818130Z"
        },
        "id": "regular-opinion",
        "outputId": "717297e2-56cc-425b-c5ea-392ec4fee1bc"
      },
      "source": [
        "attn_dm = MNISTDataModule(Dataset = AttnDataset)\n",
        "attn_dm.setup()\n",
        "imgs, labels = next(iter(attn_dm.train_dataloader()))\n",
        "imgs.shape, labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 16, 49]), torch.Size([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T08:55:12.907387Z",
          "start_time": "2021-03-24T08:55:12.622359Z"
        },
        "id": "solved-chamber",
        "outputId": "287040c1-d789-4b09-d880-d4968791e017"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        ax = plt.subplot(4, 4, i*4 + j + 1)\n",
        "        ax.imshow(imgs[6,i*4 + j].view(7, 7), cmap=\"gray\")\n",
        "        ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T08:55:12.923388Z",
          "start_time": "2021-03-24T08:55:12.909388Z"
        },
        "code_folding": [],
        "id": "respected-showcase"
      },
      "source": [
        "# basado en: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
        "import math\n",
        "\n",
        "class ScaledDotSelfAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        \n",
        "        # key, query, value projections\n",
        "        self.key = torch.nn.Linear(n_embd, n_embd)\n",
        "        self.query = torch.nn.Linear(n_embd, n_embd)\n",
        "        self.value = torch.nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, F = x.size()\n",
        "        \n",
        "        # calculate query, key, values \n",
        "        k = self.key(x) # (B, L, F)\n",
        "        q = self.query(x) # (B, L, F)\n",
        "        v = self.value(x) # (B, L, F)\n",
        "        \n",
        "        # attention (B, L, F) x (B, F, L) -> (B, L, L)\n",
        "        att = (q @ k.transpose(1, 2)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = torch.nn.functional.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, L, L) x (B, L, F) -> (B, L, F)\n",
        "        \n",
        "        return y\n",
        "\n",
        "class Model(MLP):\n",
        "\n",
        "    def __init__(self, n_embd=7*7, seq_len=4*4):\n",
        "        super().__init__()\n",
        "        self.mlp = None\n",
        "\n",
        "        self.attn = ScaledDotSelfAttention(n_embd)\n",
        "        self.actn = torch.nn.ReLU(inplace=True)\n",
        "        self.fc = torch.nn.Linear(n_embd*seq_len, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attn(x)\n",
        "        #print(x.shape)\n",
        "        y = self.fc(self.actn(x.view(x.size(0), -1)))\n",
        "        #print(y.shape)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "provincial-hudson"
      },
      "source": [
        "Nuestro modelo basado en atención tiene muchos menos parámetros. Esto es debido a que en el MLP todos las neuronas en la capa oculta están conectadas a todos los pixeles de la imagen. Ahora, sin embargo, reutilizamos conexiones a nivel de *patch*, de manera similar a cómo funcionan las redes convolucionales. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:02:28.684868Z",
          "start_time": "2021-03-24T09:01:36.138918Z"
        },
        "id": "through-authorization",
        "colab": {
          "referenced_widgets": [
            "",
            "a01e1c5a25784bc589cf8977b8bacf7e"
          ]
        },
        "outputId": "3b27015d-70ee-4a75-96ae-7cfbf002fd0c"
      },
      "source": [
        "model = Model()\n",
        "trainer = pl.Trainer(max_epochs=5, gpus=1, logger=None)\n",
        "trainer.fit(model, attn_dm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "\n",
            "  | Name | Type                   | Params\n",
            "------------------------------------------------\n",
            "0 | attn | ScaledDotSelfAttention | 7.4 K \n",
            "1 | actn | ReLU                   | 0     \n",
            "2 | fc   | Linear                 | 7.9 K \n",
            "------------------------------------------------\n",
            "15.2 K    Trainable params\n",
            "0         Non-trainable params\n",
            "15.2 K    Total params\n",
            "0.061     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a01e1c5a25784bc589cf8977b8bacf7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:04:10.982031Z",
          "start_time": "2021-03-24T09:04:10.710498Z"
        },
        "id": "operational-match",
        "outputId": "8835f72a-bdf3-4337-c03c-9830acd0c24e"
      },
      "source": [
        "import random\n",
        "\n",
        "attn_imgs, attn_labels = next(iter(attn_dm.val_dataloader()))\n",
        "preds = model.predict(attn_imgs)\n",
        "\n",
        "ix = random.randint(0, dm.batch_size)\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        ax = plt.subplot(4, 4, i*4 + j + 1)\n",
        "        ax.imshow(attn_imgs[ix,i*4 + j].view(7, 7), cmap=\"gray\")\n",
        "        ax.axis('off')\n",
        "fig.suptitle(f'{attn_labels[ix]} / {preds[ix].item()}', color=\"green\" if attn_labels[ix] == preds[ix].item() else \"red\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}