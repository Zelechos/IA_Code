# -*- coding: utf-8 -*-
"""Ejercicio2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Zelechos/IA_Code/blob/master/SegundoParcial%20IA/Ejercicio2.ipynb

# Modelo 2
## La referencias usada para realizar este model es
[Repsitorio sis421](https://github.com/cwpachecol/SIS421)
"""

# montamos nuestro drive al proyecto
from google.colab import drive
drive.mount('/content/drive')

"""# Traemos nuestro dataset"""

#hacemos una copia para trabajar con el dataset
import shutil

shutil.copy("/content/drive/MyDrive/revisiones.json","/content/revisiones.json")

"""# Realizamos el Preprocesamiento de Textos"""

import json

with open('/content/revisiones.json') as file:
    data = json.load(file)

#accedemos a la evaluation
print("evaluation : ", data["paper"][0]["review"][0]["evaluation"])

#accedemos a la text
print("text : ", data["paper"][0]["review"][0]["text"])

#averiguando la longitud
print("longitud : ", len(data["paper"][0]["review"]))

paper = data["paper"]

# Capturamos todas las reviews en una lista
Reviews = []

cont = 0
for x in paper:
  # print(x[cont]["review"])
  Reviews.append(data["paper"][cont]["review"])
  cont += 1

print(len(Reviews))
print(Reviews[171][0]["text"])

"""# Tenemos reviews vacias 51,61,105"""

# print(len(Reviews[0]))
cont = 1 
# Obtenemos todas las longitudes de nuestra Lista de Reviews
Longitudes = []
for i in Reviews:
  # print(cont ,"---->",i)
  Longitudes.append(len(i))
  cont += 1

print(Longitudes)

Evalutions = []
Texts = []

Data = []
for index in range(len(Longitudes)):
  # print(index)
  for i in range(Longitudes[index]):
    Data.append([ Reviews[index][i]["evaluation"] , Reviews[index][i]["text"]])
    # Evalutions.append(Reviews[index][i]["evaluation"])
    # Texts.append(Reviews[index][i]["text"])

print(Data[404])

"""# Contamos con 405 Textos para entrenar nuestro modelo"""

print(len(Data))

cont = 1 


for i in Data:
    # print(cont,"--> ",i[0])
    if i[0] == '-2':
      i[0] = -2
    elif i[0] == '-1':
      i[0] = -1
    elif i[0] == '0':
      i[0] = 0
    elif i[0] == '1':
      i[0] = 1
    elif i[0] == '2':
      i[0] = 2

    cont += 1

import pandas as pd
import os
Train = []
Test = []
v =[]
t =[]
for i in Data:
    v.append(i[0])
    t.append(i[1])


data = {'Valoracion': v[:300],
        'Texto': t[:300]}

data1 = {'Valoracion': v[300:],
        'Texto': t[300:]}

os.mkdir('/content/Dataset')
df = pd.DataFrame(data, columns = ['Valoracion', 'Texto'])
df1 = pd.DataFrame(data1, columns = ['Valoracion', 'Texto'])
df.to_csv('/content/Dataset/train.csv')
df1.to_csv('/content/Dataset/test.csv')

print(len(v[:300]))

import torch
import torchtext
import torch.nn as nn
import torch.optim as optim
import spacy
from torchtext.legacy.data import Field, TabularDataset, BucketIterator

device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

#python -m spacy download en
spacy_en = spacy.load("en")

def tokenize(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

Texto = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)
Valoracion = Field(sequential=False, use_vocab=False)

fields = {"Texto": ("t", Texto), "Valoracion": ("v", Valoracion)}

train_data, test_data = TabularDataset.splits(
                                        path='/content/Dataset',
                                        train='train.csv',
                                        test='test.csv',
                                        format='csv',
                                        fields=fields)

len(train_data) , len(test_data)

print(vars(train_data.examples[0]))

Texto.build_vocab(train_data, max_size=10000, min_freq=1,vectors="glove.6B.100d")

Texto.vocab.freqs.most_common(25)

Texto.vocab.itos[:10]

train_iterator, test_iterator = BucketIterator.splits(
    (train_data, test_data), batch_size=2, device=device
)

class RNN_LSTM(nn.Module):
    def __init__(self, input_size, embed_size, hidden_size, num_layers):
        super(RNN_LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(input_size, embed_size)
        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers)
        self.fc_out = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size).to(device)

        embedded = self.embedding(x)
        outputs, _ = self.rnn(embedded, (h0, c0))
        prediction = self.fc_out(outputs[-1, :, :])

        return prediction

# Hyperparameters
input_size = len(Texto.vocab)
hidden_size = 512
num_layers = 2
embedding_size = 100
learning_rate = 0.005
num_epochs = 10

# Initialize network
model = RNN_LSTM(input_size, embedding_size, hidden_size, num_layers).to(device)

# (NOT COVERED IN YOUTUBE VIDEO): Load the pretrained embeddings onto our model
pretrained_embeddings = Texto.vocab.vectors
model.embedding.weight.data.copy_(pretrained_embeddings)

# Loss and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

"""# Entrenamos la Red Neuronal"""

# Train Network

for epoch in range(num_epochs):
    
    for batch_idx, batch in enumerate(train_iterator):
        # Get data to cuda if possible
        data = batch.t.to(device=device)
        targets = batch.v.to(device=device)

        # forward
        scores = model(data)
        loss = criterion(scores.squeeze(1), targets.type_as(scores))

        # backward
        optimizer.zero_grad()
        loss.backward()

        # gradient descent
        optimizer.step()

        
        print(f"Epoch {epoch+1}/{num_epochs} loss {loss:.5f} val_loss {loss:.5f} ")
print(scores)

def predict(model, X):
    model.eval() 
    with torch.no_grad():
        X = torch.tensor(X).to(device)
        pred = model(X)
        return pred

sentences = ["en rob√≥tica con fineso", "The paper is well", "this film is good", "a waste of time"]
tokenized = [[tok.text for tok in spacy_en.tokenizer(sentence)] for sentence in sentences]
indexed = [[Texto.vocab.stoi[_t] for _t in t] for t in tokenized]
tensor = torch.tensor(indexed).permute(1,0)
predictions = torch.argmax(predict(model, tensor), axis=1)
predictions